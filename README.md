---

# **Pipeline de An谩lisis de Tendencias en la Industria Espacial**   

## **Descripci贸n del Proyecto**  
Este proyecto implementa un **pipeline de datos** utilizando la API de [Spaceflight News](https://api.spaceflightnewsapi.net/v4/docs/) para extraer, procesar y analizar informaci贸n sobre la industria espacial. Se emplea **Google Cloud Composer (Airflow)** para orquestar las tareas, **BigQuery** para almacenamiento y an谩lisis, y **Dataproc (Spark)** Alternativa para procesamiento distribuido (BigQuery con funciones SQL avanzadas)  

## **Arquitectura**  
 **Extracci贸n:** Datos de art铆culos, blogs y reportes desde la API de Spaceflight News.  
 **Procesamiento:** Limpieza, deduplicaci贸n y an谩lisis con **Apache Spark en Dataproc**.  
 **Almacenamiento:** **Google Cloud Storage (GCS)** para datos crudos y **BigQuery** para an谩lisis estructurado.  
 **Orquestaci贸n:** **Cloud Composer (Airflow)** para la ejecuci贸n automatizada del pipeline.  
 **An谩lisis:** Queries en **BigQuery** para identificar tendencias y fuentes m谩s relevantes.  

## **Tecnolog铆as Utilizadas**  
 **Extracci贸n de datos:** Python + Requests + API Spaceflight News  
 **Procesamiento:** Apache Spark sobre Dataproc (Modificaci贸n BigQuery con funciones SQL avanzadas) 
 **Almacenamiento:** Google Cloud Storage (GCS) y BigQuery  
 **Orquestaci贸n:** Cloud Composer (Airflow)  
 **An谩lisis SQL:** Queries en BigQuery  

## **Flujo del Pipeline**  
1锔 **Ingesta:** DAG de Airflow extrae datos de la API y los guarda en **GCS**.  
2锔 **Procesamiento:** Job en **Dataproc (Spark)** limpia, deduplica y clasifica los datos(Modificaci贸n BigQuery con funciones SQL avanzadas).  
3锔 **Almacenamiento:** Los datos transformados se almacenan en **BigQuery**.  
4锔 **An谩lisis:** Queries para tendencias por mes y ranking de fuentes influyentes.  
5锔 **Automatizaci贸n:** Airflow ejecuta el flujo de trabajo diariamente.  

## **Instalaci贸n y Configuraci贸n**  
### **1. Clonar el Repositorio**  
```bash
git clone https://github.com/tuusuario/spaceflight-pipeline.git
cd spaceflight-pipeline
```

### **2. Configurar Variables de Entorno**  
```bash
export PROJECT_ID="tu-proyecto-gcp"
export BUCKET_NAME="tu-bucket-gcs"
export BIGQUERY_DATASET="tu-dataset-bigquery"
export API_URL="https://api.spaceflightnewsapi.net/v4"
```

### **3. Implementar Airflow en Cloud Composer**  
1. Crear un entorno de **Cloud Composer** en GCP:  
   ```bash
   gcloud composer environments create spaceflight-pipeline \
     --location us-central1 \
     --image-version composer-2-airflow-2
   ```
2. Configurar el DAG en Airflow:  
   - Subir el archivo **`spaceflight_dag.py`** al bucket de **Cloud Composer**:  
     ```bash
     gsutil cp dags/spaceflight_dag.py gs://tu-bucket-composer/dags/
     ```
   - Verificar la ejecuci贸n en la interfaz de **Cloud Composer**.

### **4. Ejecutar el Pipeline Manualmente**  
```bash
gcloud composer environments run spaceflight-pipeline \
    --location us-central1 trigger_dag -- spaceflight_dag
```

## **Consultas SQL en BigQuery**  
Ejemplo de consulta para **tendencias de temas por mes**:
```sql
SELECT topic, COUNT(*) as cantidad, DATE_TRUNC(published_at, MONTH) as mes
FROM `tu-proyecto-gcp.tu-dataset.fact_article`
GROUP BY topic, mes
ORDER BY mes DESC, cantidad DESC;
```

## **Tareas Pendientes**  
锔 Optimizar particionamiento de BigQuery para mejorar consultas.  
锔 Implementar dashboard en Looker Studio.  
锔 Agregar m谩s m茅tricas para evaluar impacto de noticias.  

---

 **Contacto:** *juancarloscm@yahoo.com*  
Si tienes dudas o sugerencias, 隆abre un issue en el repo! 
